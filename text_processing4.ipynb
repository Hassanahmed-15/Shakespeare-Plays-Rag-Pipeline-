{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvgd/u50evuu1zALHDaitM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hassanahmed-15/Shakespeare-Plays-Rag-Pipeline-/blob/main/text_processing4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ZpiblTeBdRw9",
        "outputId": "05e54d4d-1f65-4648-87c2-1cffb8881d97"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9d411e0e-3443-465c-9024-cd5154519137\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9d411e0e-3443-465c-9024-cd5154519137\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving newvariorumediti10shak.pdf to newvariorumediti10shak.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from docx import Document\n",
        "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
        "import openai\n",
        "\n",
        "# --- SET YOUR OPENAI API KEY ---\n",
        "openai.api_key = \"\"\n",
        "\n",
        "# --- INPUT PDF ---\n",
        "pdf_path = \"/content/newvariorumediti11shak.pdf\"  # Your PDF file\n",
        "start_page =  175 #Page numbering starts at 0\n",
        "end_page =195 #Page 78 is index 77, inclusive\n",
        "\n",
        "# --- Function to extract text from PDF ---\n",
        "def extract_pdf_text(pdf_path, start_page, end_page):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for i in range(start_page, end_page + 1):\n",
        "        page = doc.load_page(i)\n",
        "        page_text = page.get_text(\"text\")\n",
        "        if page_text and page_text.strip():\n",
        "            text += page_text + \"\\n\"\n",
        "        else:\n",
        "            print(f\"Warning: Page {i+1} has no extractable text. Skipping...\")\n",
        "    return text\n",
        "\n",
        "# --- Function to call GPT-5 mini to structure notes ---\n",
        "def gpt5_structure_notes_exact(text_chunk, act_number=1):\n",
        "    prompt = f\"\"\"\n",
        "DONT YOU MISS ANY SCHOLARYLY COMMENTARY OR SKIP THEM OR SHORT THEM\n",
        "\n",
        "You must now process ACT 2, SCENE 2 in one go, with 0% chance of error dont mix the scn every scn has its own play text and scholary commentary\n",
        "can u please dont skipppppppp any scholary notes why the hell are u skipping scholarly commentary from the very first page to the very last page i want everything that is in the notes no chance of error donttt every skip anything even if u dont recognize it\n",
        "\n",
        "WHY IS THERE TWO LINE NUMBERS IM SEEING? IN SCHOLARY COMMENTARY I WANNA SEE ONLY ONE LINE NUMBER (THT NUMBER TO WHICH IT IS REFERNCING EXACTLY TO THE PLAY TEXT) i also see an error that i see ']' this bracket in scholary commentary although i need :1\n",
        "pls THIS TASK WAS FOR ACT NO 2 SCN NO 2\n",
        "\n",
        "\n",
        "THERE IS AN ERROR FIX IT WHEN U R WRITING SCHOLARY COMMENTART U R MENTIONING 2 LINE NUMBER LIKE 1: 8 : you dont neeed to put the first line number just mention tht line number which have refernce to the actual line of the scholary commentary\n",
        "\n",
        "PLS FIX BASIC SPELLING ERRORS AND MAKE IT OCR ERROR FREE\n",
        "\n",
        "TO BE CLEAR WHT I WANT IS THT AS WE R CREATING AN CUSTOM LINES FOR OUR PLAY TEXT SO WHTEVER THE SCHOLARY COMMENTART IS THERE IN OUR CUSTOM LINE NUMBER OUR CUSTOM LIN NUMBER FOR BOTH PLAY TEXT AND SCHOLARY COMMENTARY SHOULD BE THE SAME THEY SHOULD NOT BE DIFFERNET\n",
        "\n",
        " PLS DONT MERGE SCNS EVERY SCN HAS ITS OWN PLAY TEXT AND COMMENTARY\n",
        "\n",
        "I AM CONSISTENTLY SEEING SOME ERROS REGARDING THE MATCHING OF THE CUSTOM LINE NUMBER OF PLAY TEXT AND SCHOLARY COMMENTARY THEY SHOULD MATCH EXACTLY AND NO SCHOLARY COMMENTARY SHOULD BE ,MISSED FROM THE TEXT\n",
        "\n",
        "I SEE SOME ERROR IN THE LINE MATCHING ALSO IN THE FORMATTING OF THE PLAY TEXT I DONT SEE THT U R MENTIONING THE CHARACTER NAME CONSISTENTLY ALOS THE CUSTOM LINE IS NOT MATCHING\n",
        "PLS FIX THIS ERROR I DONT WANT ANY MORE ERRORS SKIPPING OR SUMMARZING\\\n",
        "\n",
        "\n",
        "Transcribe the play text exactly as printed, correcting only OCR errors (ft ‚Üí st, haue ‚Üí have).(correct all of them)\n",
        "\n",
        "Assign continuous custom line numbers starting at 1 for each scene. One line number per printed line of play text, not per sentence. Stage directions must be included inline with the nearest spoken line and numbered.\n",
        "\n",
        "Transcribe the scholarly commentary exactly as printed, correcting only OCR errors.\n",
        "\n",
        "For each commentary note, replace its original reference number with the correct custom line number from the play text. Commentary must be tied directly to the exact play line it applies to.\n",
        "\n",
        "Important formatting rules:\n",
        "\n",
        "No skipped lines, no skipped commentary. Everything must be included.\n",
        "\n",
        "Do not merge scenes.\n",
        "\n",
        "Play text first, then commentary.\n",
        "\n",
        "The commentary format must be exactly:\n",
        "\n",
        "[Line#]: [Scholar Name]: [Commentary text exactly as in edition]\n",
        "\n",
        "\n",
        "(No extra brackets, no ‚Äúsee line xxx,‚Äù no additions beyond what is printed.)\n",
        "\n",
        "Keep wording exactly as in the edition. Only fix OCR errors.\n",
        "\n",
        "Commentary numbers must match the custom line numbers in the play text exactly.\n",
        "\n",
        "Output format:\n",
        "\n",
        "ACT 2, SCENE 2\n",
        "\n",
        "=== PLAY TEXT ===\n",
        "Line# Character: [Exact line of dialogue or stage direction]\n",
        "\n",
        "=== SCHOLARLY COMMENTARY ===\n",
        "Line#: Scholar Name: Exact commentary\n",
        "\n",
        "\n",
        "Scope: Produce the entire ACT 2, SCENE 2 in this format. Nothing should be summarized, omitted, or altered.\n",
        "\n",
        "\n",
        "MAKE SUREEEEEE THAT OUR CUSTOM LINE NUMBER OF PLAY TEXT MATCHES WITH THE LINE NUMBER OF SCHOLARY COMMENTARY VERIFY IT BUT DONT WRITE ANYTHING ABT VERIFICATION IN THE TEXT I DONT NEED ANY ERROS\n",
        "\n",
        "OK WHT I WANT U IS TO PICK 10 RANDOM LINES AND SEE IF THE PLAY TEXT LINE NUMBER MATCHES WITH SCHOLARY COMMENTARY LINE NUMBER ( WHICH ARE MEANT TO BE MATCHING) AND ADD AT THE END OF THE TEXT HOW MUCH SUCCESS U GOT\n",
        "\n",
        "I WANT THE LINE NUMBER WHICH MATCHES 100 PERCENT WITH SAME LINE NUMBER WITH LAY TEXT AND SCOLARY COMMENTARY IN THE END I WANT THEM TO BE IN SAME LINE NUMBER WITH SIMILAR CONTENT\n",
        "\n",
        "one tip : to match the line number and scholarly commentary there is a word at the start of every commentary so find tht word in the play text line if tht word is there verify tht it has the same context if yes then thhey both should have the same line number\n",
        "{text_chunk}\n",
        "\n",
        "\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    structured_text = response.choices[0].message.content\n",
        "    return structured_text\n",
        "\n",
        "# --- Function to write structured text to DOCX ---\n",
        "def write_to_docx(structured_text, output_path=\"Act1_Scenes1_to_3.docx\"):\n",
        "    doc = Document()\n",
        "    for line in structured_text.split(\"\\n\"):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            doc.add_paragraph()\n",
        "            continue\n",
        "\n",
        "        # Bold headings\n",
        "        if line.startswith(\"**ACT\") or line.startswith(\"**=== \"):\n",
        "            p = doc.add_paragraph()\n",
        "            run = p.add_run(line.replace(\"**\", \"\"))\n",
        "            run.bold = True\n",
        "            p.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
        "        else:\n",
        "            doc.add_paragraph(line)\n",
        "    doc.save(output_path)\n",
        "    print(f\"DONE: Structured docx saved at {output_path}\")\n",
        "\n",
        "# --- MAIN PROCESS ---\n",
        "print(\"Extracting PDF text (pages 1-78)...\")\n",
        "pdf_text = extract_pdf_text(pdf_path, start_page, end_page)\n",
        "\n",
        "if not pdf_text.strip():\n",
        "    raise ValueError(\"No extractable text found in PDF. Check the file or page range.\")\n",
        "\n",
        "print(\"Structuring notes for Scenes 1-3 with GPT-5 mini...\")\n",
        "structured_text = gpt5_structure_notes_exact(pdf_text, act_number=1)\n",
        "\n",
        "print(\"Writing to DOCX file...\")\n",
        "write_to_docx(structured_text, \"Act2_SCN1.docx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF9pjJ0adflt",
        "outputId": "0630412f-4bb3-42aa-bdb3-94705d7bad06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting PDF text (pages 1-78)...\n",
            "Structuring notes for Scenes 1-3 with GPT-5 mini...\n",
            "Writing to DOCX file...\n",
            "DONE: Structured docx saved at Act2_SCN1.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fitz\n",
        "!pip install python-docx\n",
        "!pip install PyMuPDF\n",
        "!pip install tools\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htDQt3rAeaDi",
        "outputId": "b3d85317-af43-4d94-b1c3-01d7ebd2823d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fitz in /usr/local/lib/python3.12/dist-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.12/dist-packages (from fitz) (5.0.9)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.12/dist-packages (from fitz) (7.2.0)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.12/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (from fitz) (5.3.2)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.12/dist-packages (from fitz) (1.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fitz) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from fitz) (2.2.2)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.12/dist-packages (from fitz) (1.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from fitz) (1.16.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from httplib2->fitz) (3.2.3)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from nibabel->fitz) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel->fitz) (4.14.1)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (8.2.1)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (3.5)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (2.1.1)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (2.9.0.post0)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (7.1.4)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (3.20.1)\n",
            "Requirement already satisfied: traits>=6.2 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (7.0.2)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (3.19.1)\n",
            "Requirement already satisfied: acres in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (0.5.0)\n",
            "Requirement already satisfied: etelemetry>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (0.3.1)\n",
            "Requirement already satisfied: looseversion!=1.2 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: puremagic in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (1.30)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->fitz) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->fitz) (2025.2)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.12/dist-packages (from pyxnat->fitz) (5.4.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.12/dist-packages (from pyxnat->fitz) (2.32.4)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.12/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.12/dist-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.2->nipype->fitz) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->pyxnat->fitz) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->pyxnat->fitz) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->pyxnat->fitz) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->pyxnat->fitz) (2025.8.3)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.14.1)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.3\n",
            "Collecting tools\n",
            "  Downloading tools-1.0.4-py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading tools-1.0.4-py3-none-any.whl (39 kB)\n",
            "Installing collected packages: tools\n",
            "Successfully installed tools-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader, PdfWriter\n",
        "\n",
        "def split_pdf(input_path, output_base, splits):\n",
        "    reader = PdfReader(input_path)\n",
        "\n",
        "    for i, (start_page, end_page) in enumerate(splits, start=1):\n",
        "        writer = PdfWriter()\n",
        "        for page_num in range(start_page - 1, end_page):  # 0-indexed\n",
        "            writer.add_page(reader.pages[page_num])\n",
        "\n",
        "        output_path = f\"{output_base}_act{i}_pages_{start_page}_to_{end_page}.pdf\"\n",
        "        with open(output_path, \"wb\") as f:\n",
        "            writer.write(f)\n",
        "\n",
        "        print(f\"‚úÖ Saved: {output_path}\")\n",
        "\n",
        "# === Example usage ===\n",
        "input_pdf = \"/content/HMM.pdf\"\n",
        "\n",
        "# Define (start_page, end_page) for each Act ‚Äî you should adjust these based on your PDF structure\n",
        "splits = [\n",
        "    (250, 271),   # Act 1\n",
        "\n",
        "]\n",
        "\n",
        "split_pdf(input_pdf, \"macbeth\", splits)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhJGGdDyeaPs",
        "outputId": "03291d12-ae9d-4fce-b3af-98b8511d0987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved: macbeth_act1_pages_250_to_271.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hc4Ll_FgG8vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRngyOch9tkm",
        "outputId": "06074d56-e6c5-4fdb-e164-3fbd84b5d0c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.6/232.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from docx import Document\n",
        "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
        "import openai\n",
        "\n",
        "# --- SET YOUR OPENAI API KEY ---\n",
        "openai.api_key = \"\"\n",
        "\n",
        "# --- INPUT PDF ---\n",
        "pdf_path = \"/content/newvariorumediti11shak.pdf\"  # Your PDF file\n",
        "start_page =   460 #Page numbering starts at 0\n",
        "end_page =484 #Page 78 is index 77, inclusive\n",
        "\n",
        "# --- Function to extract text from PDF ---\n",
        "def extract_pdf_text(pdf_path, start_page, end_page):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for i in range(start_page, end_page + 1):\n",
        "        page = doc.load_page(i)\n",
        "        page_text = page.get_text(\"text\")\n",
        "        if page_text and page_text.strip():\n",
        "            text += page_text + \"\\n\"\n",
        "        else:\n",
        "            print(f\"Warning: Page {i+1} has no extractable text. Skipping...\")\n",
        "    return text\n",
        "\n",
        "# --- Function to call GPT-5 mini to structure notes ---\n",
        "def gpt5_structure_notes_exact(text_chunk, act_number=1):\n",
        "    prompt = f\"\"\"\n",
        "    RULES\n",
        "\n",
        "PLAY TEXT\n",
        "start from line 180\n",
        "\n",
        "\n",
        "Do not transcribe the play text.\n",
        "\n",
        "be consistent with the exact line number\n",
        "\n",
        "Instead, leave the play text section completely blank (but still include the header).\n",
        "\n",
        "Example:\n",
        "\n",
        "=== PLAY TEXT ===\n",
        "(blank)\n",
        "\n",
        "\n",
        "SCHOLARLY COMMENTARY TRANSCRIPTION\n",
        "\n",
        "Transcribe all scholarly notes exactly as printed, correcting only OCR errors (e.g., ‚Äúhaue‚Äù ‚Üí ‚Äúhave,‚Äù ‚Äúft‚Äù ‚Üí ‚Äúst‚Äù).\n",
        "\n",
        "No skipping, no summarizing, no omission. Every single commentary note/footnote must appear.\n",
        "\n",
        "For each commentary note:\n",
        "\n",
        "Use the custom line number that corresponds to the play text line (even though play text is blank).\n",
        "\n",
        "The note must preserve the cue word/phrase with its closing bracket (e.g., word]) exactly as in the edition, with OCR errors corrected.\n",
        "\n",
        "Format each note exactly as:\n",
        "\n",
        "[Line#]: Scholar Name: word] Commentary text exactly as printed\n",
        "\n",
        "\n",
        "Scholar‚Äôs name must always appear as printed in the edition (Capell, Johnson, Malone, Steevens, etc.).\n",
        "\n",
        "Do not drop or alter the cue word, the bracket, or the scholar‚Äôs name.\n",
        "\n",
        "NUMBER MATCHING REQUIREMENT\n",
        "\n",
        "Commentary must use custom line numbers that match exactly with the numbering system of the scene.\n",
        "\n",
        "Even though play text is blank, you must still assign continuous custom line numbers for each scene.\n",
        "\n",
        "No double numbering, no skipped numbers.\n",
        "\n",
        "OUTPUT FORMAT\n",
        "\n",
        "ACT 5, SCENE 2\n",
        "\n",
        "=== PLAY TEXT ===\n",
        "(blank)\n",
        "\n",
        "=== SCHOLARLY COMMENTARY ===\n",
        "Line#: Scholar Name: word] Commentary text\n",
        "\n",
        "\n",
        "VERIFICATION REQUIREMENT\n",
        "\n",
        "At the end of the scene, verify that no commentary or footnotes are missing.\n",
        "\n",
        "Output a summary like:\n",
        "\n",
        "=== VERIFICATION SUMMARY ===\n",
        "100% success: All scholarly commentary included with cue words, scholar names, and matching custom line numbers.\n",
        "\n",
        "SCOPE\n",
        "\n",
        "Apply this to ACT [5], SCENE [2].\n",
        "\n",
        "\n",
        "\n",
        "every act has its own play text and scholarly commentary dont merge scn\n",
        "\n",
        "Continue until the end of the scene.\n",
        "\n",
        "Do not merge scenes together.\n",
        "\n",
        "No commentary is to be omitted, cut, or summarized.\n",
        "\n",
        "üëâ This ensures you get only scholarly commentary, fully preserved, with:\n",
        "\n",
        "Custom line numbers\n",
        "\n",
        "Cue words (word]) always present\n",
        "\n",
        "Scholar names always included\n",
        "\n",
        "No missing notes\n",
        "{text_chunk}\n",
        "\n",
        "\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    structured_text = response.choices[0].message.content\n",
        "    return structured_text\n",
        "\n",
        "# --- Function to write structured text to DOCX ---\n",
        "def write_to_docx(structured_text, output_path=\"Act1_Scenes1_to_3.docx\"):\n",
        "    doc = Document()\n",
        "    for line in structured_text.split(\"\\n\"):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            doc.add_paragraph()\n",
        "            continue\n",
        "\n",
        "        # Bold headings\n",
        "        if line.startswith(\"**ACT\") or line.startswith(\"**=== \"):\n",
        "            p = doc.add_paragraph()\n",
        "            run = p.add_run(line.replace(\"**\", \"\"))\n",
        "            run.bold = True\n",
        "            p.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
        "        else:\n",
        "            doc.add_paragraph(line)\n",
        "    doc.save(output_path)\n",
        "    print(f\"DONE: Structured docx saved at {output_path}\")\n",
        "\n",
        "# --- MAIN PROCESS ---\n",
        "print(\"Extracting PDF text (pages 1-78)...\")\n",
        "pdf_text = extract_pdf_text(pdf_path, start_page, end_page)\n",
        "\n",
        "if not pdf_text.strip():\n",
        "    raise ValueError(\"No extractable text found in PDF. Check the file or page range.\")\n",
        "\n",
        "print(\"Structuring notes for Scenes 1-3 with GPT-5 mini...\")\n",
        "structured_text = gpt5_structure_notes_exact(pdf_text, act_number=1)\n",
        "\n",
        "print(\"Writing to DOCX file...\")\n",
        "write_to_docx(structured_text, \"Act1_SCN1.docx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA7AhOEDG9mf",
        "outputId": "e888ef6c-283e-41ae-95d8-6970176e0032"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting PDF text (pages 1-78)...\n",
            "Structuring notes for Scenes 1-3 with GPT-5 mini...\n",
            "Writing to DOCX file...\n",
            "DONE: Structured docx saved at Act1_SCN1.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from docx import Document\n",
        "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
        "import openai\n",
        "\n",
        "# --- SET YOUR OPENAI API KEY ---\n",
        "openai.api_key = \"\"\n",
        "\n",
        "# --- INPUT PDF ---\n",
        "pdf_path = \"/content/34321.pdf\"  # Your PDF file\n",
        "start_page =  0 #Page numbering starts at 0\n",
        "end_page =15 #Page 78 is index 77, inclusive\n",
        "\n",
        "# --- Function to extract text from PDF ---\n",
        "def extract_pdf_text(pdf_path, start_page, end_page):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for i in range(start_page, end_page + 1):\n",
        "        page = doc.load_page(i)\n",
        "        page_text = page.get_text(\"text\")\n",
        "        if page_text and page_text.strip():\n",
        "            text += page_text + \"\\n\"\n",
        "        else:\n",
        "            print(f\"Warning: Page {i+1} has no extractable text. Skipping...\")\n",
        "    return text\n",
        "\n",
        "# --- Function to call GPT-5 mini to structure notes ---\n",
        "def gpt5_structure_notes_exact(text_chunk, act_number=1):\n",
        "    prompt = f\"\"\"\n",
        "   You are a precise text editor. I will paste a document that contains two sections marked exactly as below:\n",
        "\n",
        "   you are not deleting the wrong line number replace it with tht line numberm\n",
        "\n",
        "=== PLAY TEXT ===\n",
        "... numbered play lines like:\n",
        "1 King: ...\n",
        "2 Moreover that we much did long to see you,\n",
        "...\n",
        "\n",
        "=== SCHOLARLY COMMENTARY ===\n",
        "... numbered commentary entries like:\n",
        "27: Some editor note with a keyword before a bracket like Rosencrantz] Explanation...\n",
        "28: Another note ...\n",
        "\n",
        "TASK (must follow exactly):\n",
        "1. Do NOT change any text except the numeric labels at the start of commentary entries.\n",
        "   - A commentary entry's first paragraph always begins with a number, then a colon (e.g. `27:`).\n",
        "   - Replace only that leading number (keep the colon and the rest of the paragraph unchanged).\n",
        "2. For each commentary entry, locate its KEYWORD as follows:\n",
        "   - Take the text immediately before the first closing bracket `]` in that commentary entry, and extract the **last word** from that text. That last word is the KEYWORD (example: in `... Rosencrantz]` the keyword is `Rosencrantz`).\n",
        "   - If the entry has no `]`, use the last word of the entry‚Äôs first line as the KEYWORD.\n",
        "3. Find in the PLAY TEXT the line whose text contains that KEYWORD as a whole word (case-insensitive).\n",
        "   - If there is exactly one such line, use that line‚Äôs number as the new label.\n",
        "   - If there are multiple matching lines, choose the one whose line-number is **closest** to the commentary‚Äôs original number (minimize absolute difference). If there is still a tie, choose the smaller line number.\n",
        "   - If no match is found, leave that commentary label unchanged.\n",
        "4. Apply this to every commentary entry. Do not move, delete, or change any other content or any punctuation/spacing.\n",
        "5. Return the **entire document** (both sections) in the exact same format and ordering as the input, with only the commentary leading numbers updated where matches were found.\n",
        "6. Output must be plain text only. Do NOT add any explanations, footnotes, summaries, or extra blank lines. No JSON, no markup ‚Äî exactly the full document text.\n",
        "\n",
        "Now I will paste the document. Process it and return the full corrected document.\n",
        "{text_chunk}\n",
        "\n",
        "\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    structured_text = response.choices[0].message.content\n",
        "    return structured_text\n",
        "\n",
        "# --- Function to write structured text to DOCX ---\n",
        "def write_to_docx(structured_text, output_path=\"Act1_Scenes1_to_3.docx\"):\n",
        "    doc = Document()\n",
        "    for line in structured_text.split(\"\\n\"):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            doc.add_paragraph()\n",
        "            continue\n",
        "\n",
        "        # Bold headings\n",
        "        if line.startswith(\"**ACT\") or line.startswith(\"**=== \"):\n",
        "            p = doc.add_paragraph()\n",
        "            run = p.add_run(line.replace(\"**\", \"\"))\n",
        "            run.bold = True\n",
        "            p.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
        "        else:\n",
        "            doc.add_paragraph(line)\n",
        "    doc.save(output_path)\n",
        "    print(f\"DONE: Structured docx saved at {output_path}\")\n",
        "\n",
        "# --- MAIN PROCESS ---\n",
        "print(\"Extracting PDF text (pages 1-78)...\")\n",
        "pdf_text = extract_pdf_text(pdf_path, start_page, end_page)\n",
        "\n",
        "if not pdf_text.strip():\n",
        "    raise ValueError(\"No extractable text found in PDF. Check the file or page range.\")\n",
        "\n",
        "print(\"Structuring notes for Scenes 1-3 with GPT-5 mini...\")\n",
        "structured_text = gpt5_structure_notes_exact(pdf_text, act_number=1)\n",
        "\n",
        "print(\"Writing to DOCX file...\")\n",
        "write_to_docx(structured_text, \"Act1_SCN1.docx\")\n"
      ],
      "metadata": {
        "id": "mYBKBN2j9LT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fitz\n",
        "!pip install python-docx\n",
        "!pip install PyMuPDF\n",
        "!pip install tools"
      ],
      "metadata": {
        "id": "j-QPwG6K9Qdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}